{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9293937,"sourceType":"datasetVersion","datasetId":5623492},{"sourceId":9293943,"sourceType":"datasetVersion","datasetId":5623636}],"dockerImageVersionId":30763,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade transformers datasets bitsandbytes scikit-learn textstat language_tool_python matplotlib peft pandas sentence-transformers torch","metadata":{"execution":{"iopub.status.busy":"2024-09-02T04:31:48.995526Z","iopub.status.idle":"2024-09-02T04:31:48.995924Z","shell.execute_reply.started":"2024-09-02T04:31:48.995742Z","shell.execute_reply":"2024-09-02T04:31:48.995762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\nfrom datasets import Dataset\nfrom peft import get_peft_model, LoraConfig\nimport torch\n\ndef setup_environment():\n    cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub\")\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n    return cache_dir\n\ndef check_model_and_tokenizer(model_name):\n    try:\n        tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n        \n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n            tokenizer.pad_token_id = tokenizer.eos_token_id\n            print(\"Pad token set to EOS token.\")\n        \n        model = GPT2LMHeadModel.from_pretrained(model_name)\n        \n        lora_config = LoraConfig(\n            r=8,\n            lora_alpha=16,\n            lora_dropout=0.1\n        )\n        model = get_peft_model(model, lora_config)\n        \n        print(f\"Model and tokenizer for '{model_name}' loaded successfully.\")\n        return tokenizer, model\n    except Exception as e:\n        print(f\"Error loading model or tokenizer: {e}\")\n        return None, None\n\ndef check_data_format(dataset_path):\n    try:\n        with open(dataset_path, 'r') as file:\n            data = json.load(file)\n        \n        if not isinstance(data, list) or not all(isinstance(item, dict) for item in data):\n            raise ValueError(\"Data should be a list of dictionaries.\")\n        \n        required_fields = {'question', 'answer'}\n        for item in data:\n            if not required_fields.issubset(item.keys()):\n                raise ValueError(f\"Missing required fields in item: {item}\")\n\n        print(\"Data format is valid.\")\n        return data\n    except Exception as e:\n        print(f\"Error checking data format: {e}\")\n        return None\n\ndef preprocess_function(examples, tokenizer, max_length=512):\n    # Tokenize questions and answers with consistent padding and truncation\n    inputs = tokenizer(examples['question'], max_length=max_length, truncation=True, padding='max_length')\n    targets = tokenizer(examples['answer'], max_length=max_length, truncation=True, padding='max_length')\n    \n    # Ensure consistent length\n    model_inputs = {k: torch.tensor(v) for k, v in inputs.items()}\n    model_inputs['labels'] = torch.tensor(targets['input_ids'])\n    \n    return model_inputs\n\ndef preprocess_and_train_model(tokenizer, model, data, cache_dir, hyperparameters=None):\n    if hyperparameters is None:\n        hyperparameters = {\n            'batch_size': 8,\n            'num_epochs': 5,\n            'learning_rate': 2e-5,\n            'warmup_steps': 1000,\n            'weight_decay': 0.01,\n            'logging_steps': 100,\n            'save_steps': 5000,\n            'eval_steps': 1000\n        }\n\n    df = pd.DataFrame(data)\n    dataset = Dataset.from_pandas(df)\n    \n    # Tokenize and preprocess data\n    tokenized_dataset = dataset.map(lambda x: preprocess_function(x, tokenizer), batched=True, remove_columns=['question', 'answer'])\n\n    # Split dataset into training and validation sets\n    split = tokenized_dataset.train_test_split(test_size=0.1)\n    train_dataset = split['train']\n    eval_dataset = split['test']\n\n    # Define training arguments\n    training_args = TrainingArguments(\n        per_device_train_batch_size=hyperparameters.get('batch_size', 8),\n        per_device_eval_batch_size=hyperparameters.get('batch_size', 8),\n        output_dir=os.path.join(cache_dir, 'results'),\n        num_train_epochs=hyperparameters.get('num_epochs', 5),\n        learning_rate=hyperparameters.get('learning_rate', 2e-5),\n        warmup_steps=hyperparameters.get('warmup_steps', 1000),\n        weight_decay=hyperparameters.get('weight_decay', 0.01),\n        logging_dir=os.path.join(cache_dir, 'logs'),\n        logging_steps=hyperparameters.get('logging_steps', 100),\n        save_steps=hyperparameters.get('save_steps', 5000),\n        save_total_limit=5,\n        evaluation_strategy=\"steps\",\n        eval_steps=hyperparameters.get('eval_steps', 1000),\n        load_best_model_at_end=True,\n        metric_for_best_model=\"loss\",\n        greater_is_better=False,\n        report_to=\"tensorboard\",\n        fp16=True,\n        remove_unused_columns=False\n    )\n\n    # Initialize the data collator\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm=False\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        tokenizer=tokenizer,\n        data_collator=data_collator\n    )\n\n    trainer.train()\n\n    fine_tuned_model_dir = os.path.join(cache_dir, 'fine-tuned-gpt2')\n    model.save_pretrained(fine_tuned_model_dir)\n    tokenizer.save_pretrained(fine_tuned_model_dir)\n    print(f\"Fine-tuned model saved to {fine_tuned_model_dir}\")\n\ndef main():\n    model_name = \"gpt2\"\n    dataset_path = '/kaggle/input/tuning/tuning.json'\n    cache_dir = setup_environment()\n    \n    tokenizer, model = check_model_and_tokenizer(model_name)\n    \n    if tokenizer and model:\n        data = check_data_format(dataset_path)\n        \n        if data:\n            preliminary_tuning = input(\"Do you want to perform preliminary fine-tuning with a subset of the data? (yes/no): \").strip().lower()\n            if preliminary_tuning == 'yes':\n                print(\"Performing preliminary fine-tuning.\")\n                preprocess_and_train_model(tokenizer, model, data[:10], cache_dir)  # Use subset for preliminary tuning\n            \n            proceed = input(\"Do you want to proceed with full fine-tuning? (yes/no): \").strip().lower()\n            if proceed == 'yes':\n                print(\"Proceeding to full fine-tuning.\")\n                preprocess_and_train_model(tokenizer, model, data, cache_dir)\n            else:\n                print(\"Fine-tuning aborted.\")\n        else:\n            print(\"Data format is not valid. Aborting.\")\n    else:\n        print(\"Model or tokenizer failed to load. Aborting.\")\n\nif __name__ == '__main__':\n    main()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T04:31:26.444920Z","iopub.execute_input":"2024-09-02T04:31:26.445186Z","iopub.status.idle":"2024-09-02T04:31:48.994668Z","shell.execute_reply.started":"2024-09-02T04:31:26.445155Z","shell.execute_reply":"2024-09-02T04:31:48.993254Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_peft_model, LoraConfig\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup_environment\u001b[39m():\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'peft'"],"ename":"ModuleNotFoundError","evalue":"No module named 'peft'","output_type":"error"}]},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport textstat\nimport language_tool_python\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n# Define the cache directory and model paths\ncache_dir = os.path.expanduser(\"~/.cache/huggingface/hub\")\nmodel_name = \"gpt2\"  # Using base GPT-2 model for testing\nfine_tuned_model_dir = os.path.join(cache_dir, 'fine-tuned-gpt2')\n\n# Load tokenizer and model\ntokenizer = GPT2Tokenizer.from_pretrained(model_name, cache_dir=cache_dir)\nmodel = GPT2LMHeadModel.from_pretrained(model_name, cache_dir=cache_dir)  # Changed to base model for testing\n\n# Set pad_token_id if not set\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\n# Initialize language tool for grammar checking\nlanguage_tool = language_tool_python.LanguageTool('en-US')\n\ndef generate_text(prompt, max_tokens=200):\n    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, return_attention_mask=True)\n    outputs = model.generate(\n        input_ids=inputs['input_ids'],\n        attention_mask=inputs['attention_mask'],\n        max_length=max_tokens + len(inputs['input_ids'][0]),\n        num_return_sequences=1,\n        temperature=0.7,\n        top_p=0.85,\n        eos_token_id=tokenizer.eos_token_id,\n        pad_token_id=tokenizer.pad_token_id,\n        no_repeat_ngram_size=2,\n        repetition_penalty=1.2\n    )\n    \n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Debug print statements\n    print(\"Prompt:\")\n    print(prompt)\n    print(\"\\nGenerated Text:\")\n    print(generated_text)\n    print(\"-\" * 50)\n\n    if generated_text.startswith(prompt):\n        generated_text = generated_text[len(prompt):].strip()\n    \n    return generated_text\n\ndef compute_similarity(text1, text2):\n    vectorizer = TfidfVectorizer().fit([text1, text2])\n    vectors = vectorizer.transform([text1, text2])\n    return cosine_similarity(vectors[0:1], vectors[1:2])[0][0]\n\ndef check_grammar(text):\n    matches = language_tool.check(text)\n    return len(matches)  # Number of grammar issues\n\ndef evaluate_performance(data_file, max_tokens=50):\n    with open(data_file, 'r') as file:\n        data = json.load(file)\n    \n    similarities = []\n    grammar_errors = []\n    lengths = []\n    deviations = []\n\n    for item in data:\n        prompt = item['question']\n        expected_answer = item['answer']\n        generated_text = generate_text(prompt, max_tokens)\n        text_length = len(generated_text.split())\n        \n        similarity = compute_similarity(expected_answer, generated_text)\n        grammar_error_count = check_grammar(generated_text)\n        deviation = abs(len(expected_answer.split()) - text_length)\n        \n        similarities.append(similarity)\n        grammar_errors.append(grammar_error_count)\n        lengths.append(text_length)\n        deviations.append(deviation)\n\n    # Plot the metrics\n    plt.figure(figsize=(15, 10))\n\n    # Similarity Plot\n    plt.subplot(2, 2, 1)\n    plt.hist(similarities, bins=20, color='green', edgecolor='black')\n    plt.xlabel('Similarity Score')\n    plt.ylabel('Frequency')\n    plt.title('Distribution of Similarity Scores')\n\n    # Grammar Errors Plot\n    plt.subplot(2, 2, 2)\n    plt.hist(grammar_errors, bins=20, color='red', edgecolor='black')\n    plt.xlabel('Number of Grammar Errors')\n    plt.ylabel('Frequency')\n    plt.title('Distribution of Grammar Errors')\n\n    # Text Length Plot\n    plt.subplot(2, 2, 3)\n    plt.hist(lengths, bins=20, color='blue', edgecolor='black')\n    plt.xlabel('Length of Generated Text (words)')\n    plt.ylabel('Frequency')\n    plt.title('Distribution of Generated Text Lengths')\n\n    # Deviation Plot\n    plt.subplot(2, 2, 4)\n    plt.hist(deviations, bins=20, color='purple', edgecolor='black')\n    plt.xlabel('Deviation from Expected Answer Length')\n    plt.ylabel('Frequency')\n    plt.title('Distribution of Answer Deviation')\n\n    plt.tight_layout()\n    plt.savefig('performance_metrics.png')  # Save the plot as a PNG file\n    plt.show()  # Display the plot\n\n    # Explanations for the plots\n    print(\"\\nPlot Explanations:\")\n    print(\"1. **Distribution of Similarity Scores**: This plot shows how similar the generated answers are to the expected answers based on cosine similarity scores. Higher scores indicate better similarity. If the scores are generally low, the model's responses might not be closely matching the expected answers.\")\n    print(\"2. **Distribution of Grammar Errors**: This histogram illustrates the number of grammar errors detected in the generated texts. Fewer errors suggest better grammatical quality. A high number of errors may indicate issues with the model's ability to generate grammatically correct text.\")\n    print(\"3. **Distribution of Generated Text Lengths**: This plot represents the lengths of the generated texts in terms of word count. It helps to understand the verbosity of the generated answers. If the lengths vary significantly, it could mean the model is generating excessively short or long responses.\")\n    print(\"4. **Distribution of Answer Deviation**: This shows how much the length of the generated text deviates from the expected answer length. Smaller deviations indicate more precise text generation. Larger deviations might suggest that the model is not generating responses with appropriate length.\")\n\n    # Overall analysis\n    avg_similarity = np.mean(similarities)\n    avg_grammar_errors = np.mean(grammar_errors)\n    avg_length = np.mean(lengths)\n    avg_deviation = np.mean(deviations)\n\n    print(\"\\nOverall Performance Analysis:\")\n    print(f\"Average Similarity Score: {avg_similarity:.2f}\")\n    print(f\"Average Number of Grammar Errors: {avg_grammar_errors:.2f}\")\n    print(f\"Average Length of Generated Text: {avg_length:.2f} words\")\n    print(f\"Average Deviation from Expected Answer Length: {avg_deviation:.2f} words\")\n\n    if avg_similarity < 0.5:\n        print(\"The model's generated responses are generally not similar to the expected answers. Improvement in the model's training or fine-tuning might be required.\")\n    else:\n        print(\"The model's generated responses are fairly similar to the expected answers.\")\n\n    if avg_grammar_errors > 5:\n        print(\"The generated texts have a high number of grammar errors. Enhancing the model's ability to generate grammatically correct sentences could be beneficial.\")\n    else:\n        print(\"The generated texts have a relatively low number of grammar errors.\")\n\n    if avg_deviation > 10:\n        print(\"The model's responses have a significant deviation in length compared to expected answers. Adjusting the model's parameters or prompt length may help in generating more appropriately sized responses.\")\n    else:\n        print(\"The model's responses are of appropriate length compared to expected answers.\")\n\n    results = []\n    for item in data:\n        prompt = item['question']\n        expected_answer = item['answer']\n        generated_text = generate_text(prompt, max_tokens)\n        results.append({\n            'prompt': prompt,\n            'expected_answer': expected_answer,\n            'generated_text': generated_text\n        })\n    \n    return results\n\n# Path to the JSON file containing questions and answers\ndata_file = '/kaggle/input/t-small/t-small.json'\n\n# Evaluate the performance\nperformance_results = evaluate_performance(data_file)\n\n# Print the results\nfor result in performance_results:\n    print(\"\\n\" + \"=\"*50)\n    print(f\"Prompt:\")\n    print(result['prompt'])\n    print(f\"\\nExpected Answer:\")\n    print(result['expected_answer'])\n    print(f\"\\nGenerated Text:\")\n    print(result['generated_text'])\n    print(\"=\"*50)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-01T12:40:43.744902Z","iopub.execute_input":"2024-09-01T12:40:43.745315Z","iopub.status.idle":"2024-09-01T12:41:46.389148Z","shell.execute_reply.started":"2024-09-01T12:40:43.745273Z","shell.execute_reply":"2024-09-01T12:41:46.388115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\nfrom datasets import Dataset\nfrom peft import get_peft_model, LoraConfig\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport numpy as np\nfrom transformers import pipeline\n\ndef setup_environment():\n    cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub\")\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n    return cache_dir\n\ndef check_model_and_tokenizer(model_name):\n    try:\n        tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n        \n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n            tokenizer.pad_token_id = tokenizer.eos_token_id\n            print(\"Pad token set to EOS token.\")\n        \n        model = GPT2LMHeadModel.from_pretrained(model_name)\n        \n        lora_config = LoraConfig(\n            r=8,\n            lora_alpha=16,\n            lora_dropout=0.1\n        )\n        model = get_peft_model(model, lora_config)\n        \n        print(f\"Model and tokenizer for '{model_name}' loaded successfully.\")\n        return tokenizer, model\n    except Exception as e:\n        print(f\"Error loading model or tokenizer: {e}\")\n        return None, None\n\ndef check_data_format(dataset_path):\n    try:\n        with open(dataset_path, 'r') as file:\n            data = json.load(file)\n        \n        if not isinstance(data, list) or not all(isinstance(item, dict) for item in data):\n            raise ValueError(\"Data should be a list of dictionaries.\")\n        \n        required_fields = {'question', 'answer'}\n        for item in data:\n            if not required_fields.issubset(item.keys()):\n                raise ValueError(f\"Missing required fields in item: {item}\")\n\n        print(\"Data format is valid.\")\n        return data\n    except Exception as e:\n        print(f\"Error checking data format: {e}\")\n        return None\n\ndef preprocess_function(examples, tokenizer):\n    inputs = tokenizer(examples['question'], max_length=512, truncation=True, padding='max_length', return_tensors=\"pt\")\n    targets = tokenizer(examples['answer'], max_length=512, truncation=True, padding='max_length', return_tensors=\"pt\")\n    \n    inputs['input_ids'] = inputs['input_ids'].squeeze(0).tolist()\n    targets['input_ids'] = targets['input_ids'].squeeze(0).tolist()\n\n    model_inputs = {k: v for k, v in inputs.items()}\n    model_inputs['labels'] = targets['input_ids']\n    return model_inputs\n\ndef analyze_model_performance(model, tokenizer, data):\n    # Create a small sample of data for testing\n    test_sample = data[:5]\n    test_dataset = Dataset.from_dict({\n        'question': [item['question'] for item in test_sample],\n        'answer': [item['answer'] for item in test_sample]\n    })\n    test_dataset = test_dataset.map(lambda x: preprocess_function(x, tokenizer), batched=True)\n    \n    text_generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n    \n    print(\"Model Performance Analysis:\")\n    for item in test_sample:\n        question = item['question']\n        expected_answer = item['answer']\n        generated_answer = text_generator(question, max_length=50, num_return_sequences=1)[0]['generated_text']\n        \n        print(f\"Question: {question}\")\n        print(f\"Expected Answer: {expected_answer}\")\n        print(f\"Generated Answer: {generated_answer}\")\n        print()\n\ndef suggest_hyperparameters_and_tokenizer_settings(data):\n    # Hypothetical evaluation function to suggest improvements\n    num_samples = len(data)\n    avg_length = np.mean([len(item['question']) for item in data])\n    max_length = max([len(item['question']) for item in data])\n    \n    # Suggested hyperparameters based on typical scenarios\n    suggested_batch_size = 4\n    suggested_num_epochs = 5\n    suggested_learning_rate = 3e-5\n    suggested_max_length = min(max_length, 512)\n    \n    print(f\"Suggested Hyperparameters:\")\n    print(f\"Batch Size: {suggested_batch_size}\")\n    print(f\"Number of Epochs: {suggested_num_epochs}\")\n    print(f\"Learning Rate: {suggested_learning_rate}\")\n    print(f\"Maximum Token Length: {suggested_max_length}\")\n\ndef main():\n    model_name = \"gpt2\"\n    dataset_path = '/kaggle/input/t-small/t-small.json'\n    cache_dir = setup_environment()\n    \n    tokenizer, model = check_model_and_tokenizer(model_name)\n    \n    if tokenizer and model:\n        data = check_data_format(dataset_path)\n        \n        if data:\n            analyze_model_performance(model, tokenizer, data)\n            suggest_hyperparameters_and_tokenizer_settings(data)\n        else:\n            print(\"Data format is not valid. Aborting.\")\n    else:\n        print(\"Model or tokenizer failed to load. Aborting.\")\n\nif __name__ == '__main__':\n    main()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-01T13:01:27.961668Z","iopub.execute_input":"2024-09-01T13:01:27.962305Z","iopub.status.idle":"2024-09-01T13:01:38.838181Z","shell.execute_reply.started":"2024-09-01T13:01:27.962261Z","shell.execute_reply":"2024-09-01T13:01:38.837147Z"},"trusted":true},"execution_count":null,"outputs":[]}]}